{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "实验一 手写数字识别\n",
    "\n",
    "一、实验目的\n",
    "\n",
    "1.掌握卷积神经网络基本原理；\n",
    "\n",
    "2.掌握Tensorflow的基本用法以及构建卷积网络的基本操作；\n",
    "\n",
    "3.了解Tensorflow在GPU上的使用方法。\n",
    "\n",
    "二、实验要求\n",
    "\n",
    "1.搭建Tensorflow环境；\n",
    "\n",
    "2.构建一个规范的卷积神经网络组织结构；\n",
    "\n",
    "3.在MNIST手写数字数据集上进行训练和评估。\n",
    "\n",
    "三、实验原理\n",
    "\n",
    "1.TensorFlow基本用法：\n",
    "\n",
    "使用 TensorFlow, 必须了解TensorFlow:\n",
    "\n",
    "    使用图(graph) 来表示计算任务。\n",
    "    在被称之为会话 (Session) 的上下文 (context) 中执行图。\n",
    "    使用 tensor 表示数据。\n",
    "    通过 变量 (Variable) 维护状态。\n",
    "    使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或从中获取数据。\n",
    "TensorFlow 是一个编程系统, 使用图来表示计算任务。图中的节点被称之为 op (operation 的缩写)。一个 op 获得 0 个或多个 Tensor, 执行计算, 产生 0 个或多个 Tensor。每个 Tensor 是一个类型化的多维数组。例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels]。\n",
    "\n",
    "一个 TensorFlow 图描述了计算的过程。为了进行计算, 图必须在“会话”里被启动。 “会话“将图的 op 分发到诸如 CPU 或 GPU 之类的设备上, 同时提供执行op的方法。 这些方法执行后, 将产生的tensor返回。在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象; 在 C 和 C++ 语言中, 返回的 tensor 是 tensorflow::Tensor 实例。\n",
    "\n",
    "2.卷积神经网络：\n",
    "\n",
    "典型的卷积神经网络由卷积层、池化层、激活函数层交替组合构成，因此可将其视为一种层次模型，形象地体现了深度学习中“深度”之所在。\n",
    "\n",
    "卷积操作\n",
    "\n",
    "卷积运算是卷积神经网络的核心操作，给定二维的图像I作为输入，二维卷积核K， 卷积运算可表示为：\n",
    "              \t     ![公式](img/gongshi1.png)\t(1)\n",
    "                        \n",
    "给定5×5输入矩阵、3×3卷积核，相应的卷积操作如图1所示。\n",
    "  ![卷积运算](img/img1.png)\n",
    "图1 卷积运算\n",
    "\n",
    "在使用TensorFlow等深度学习框架时，卷积层会有padding参数，常用的有两种选择，一个是“valid”，一个是“same”。前者是不进行填充，后者则是进行数据填充并保证输出与输入具有相同尺寸。\n",
    "\n",
    "构建卷积或池化神经网络时，卷积步长也是一个很重要的基本参数。它控制了每个操作在特征图上的执行间隔。\n",
    " ![池化操作](img/img2.png)\n",
    "池化操作\n",
    "\n",
    "池化操作使用某位置相邻输出的总体统计特征作为该位置的输出，常用最大池化（max-pooling）和均值池化（average-pooling）。池化层不包含需要训练学习的参数，仅需指定池化操作的核大小、操作步长以及池化类型。池化操作示意如图2所示。\n",
    "\n",
    "图2 池化操作\n",
    "\n",
    "激活函数层\n",
    "\n",
    "卷积操作可视为对输入数值进行线性计算发挥线性映射的作用。激活函数的引入，则增强了深度网络的非线性表达能力，从而提高了模型的学习能力。常用的激活函数有sigmoid、tanh和ReLU函数。\n",
    "\n",
    "四、实验所用工具及数据集\n",
    "\n",
    "1.工具\n",
    "\n",
    "Anaconda、TensorFlow\n",
    "\n",
    "（Tensorflow安装教程参考：Tensorflow官网、Tensorflow中文社区、https://github.com/tensorflow/tensorflow）\n",
    "\n",
    "2.数据集\n",
    "\n",
    "MNIST手写数字数据集\n",
    "（下载地址及相关介绍：http://yann.lecun.com/exdb/mnist/）\n",
    "\n",
    "五、实验步骤与方法\n",
    "\n",
    "1）安装实验环境，包括Anaconda、TensorFlow（建议安装GPU版本），若使用GPU版本还需要安装cuda、cudnn；\n",
    "\n",
    "2）下载MNIST手写数字数据集；\n",
    "\n",
    "3）加载MNIST数据；\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "IMAGE_SIZE = 28 \n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "\n",
    "def data_type():\n",
    "  \"\"\"返回activations, weights和 placeholder变量的类型.\"\"\"\n",
    "  return tf.float32\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "  \"\"\"\n",
    "      调整为4维张量 [image index, y, x, channels].\n",
    "      像素值从[0, 255] 调整到 [-0.5, 0.5].\n",
    "  \"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(16) #每个像素存储在文件中的大小为16bits\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "    data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH #将像素值从[0,255]调整到[-0.5，0.5]\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS) # 调整数据的形状\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  \"\"\"调整label标签为向量int64 label IDs.\"\"\"\n",
    "  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64) # 调整label的形状\n",
    "  return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载训练数据、测试数据和标签。从压缩文件中获取数据，并存储为numpy数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./dataset/mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./dataset/mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./dataset/mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./dataset/mnist/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train_data = extract_data(\"./dataset/mnist/data/train-images-idx3-ubyte.gz\", 60000)\n",
    "train_labels = extract_labels(\"./dataset/mnist/data/train-labels-idx1-ubyte.gz\", 60000)\n",
    "test_data = extract_data(\"./dataset/mnist/data/t10k-images-idx3-ubyte.gz\", 10000)\n",
    "test_labels = extract_labels(\"./dataset/mnist/data/t10k-labels-idx1-ubyte.gz\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "生成验证集validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VALIDATION_SIZE = 5000  # validation set的大小.\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...] #将训练集中的前5000个数据作为验证集\n",
    "validation_labels = train_labels[:VALIDATION_SIZE] #获取对应的训练标签label\n",
    "train_data = train_data[VALIDATION_SIZE:, ...] #将剩余的数据作为训练数据\n",
    "train_labels = train_labels[VALIDATION_SIZE:] # 获取对应的标签label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4）构建模型计算图；\n",
    "\n",
    "创建输入占位符：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anaconda3\\soft\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#训练样本和标签被喂给graph\n",
    "#这些占位符节点placeholder node使用下面Run()调用的{feed_dict}参数\n",
    "#在每个training step被输入一批batch训练数据\n",
    "import tensorflow  as tf\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EVAL_BATCH_SIZE = 64\n",
    "# 这里并不实际运行计算，只是使用placeholder构建结构\n",
    "train_data_node = tf.placeholder( tf.float32,\n",
    "      shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(np.int64, shape=(BATCH_SIZE,))\n",
    "eval_data = tf.placeholder(tf.float32,\n",
    "      shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建初始化变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 下面的变量包含所有可训练的权重。\n",
    "#当调用tf.global_variables_initializer().run()}时,它们会被初始化：  \n",
    "\n",
    "SEED = 34543\n",
    "NUM_LABELS  = 10\n",
    "\n",
    "# tf.Variable创建需要初始值的变量\n",
    "# tf.truncated_normal从截断的正态分布中输出随机值。\n",
    "# 生成的值服从具有指定平均值和标准偏差的正态分布，如果生成的值大于平均值2个标准偏差的值则丢弃重新选择。\n",
    "conv1_weights = tf.Variable(\n",
    "      tf.truncated_normal([5, 5, NUM_CHANNELS, 32],  # 5x5 filter, 输出的深度（kernel的数量） 32.\n",
    "                          stddev=0.1, #标准差，正态分布\n",
    "                          seed=SEED, dtype=data_type())) #seed: 一个整数，当设置之后，每次生成的随机数都一样。\n",
    "conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n",
    "conv2_weights = tf.Variable(tf.truncated_normal( #5×5×32，输出的深度（kernel）大小为64\n",
    "      [5, 5, 32, 64], stddev=0.1,\n",
    "      seed=SEED, dtype=data_type()))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n",
    "fc1_weights = tf.Variable(  # fully connected, depth（输出） 512.\n",
    "      tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512], # 这里4D矩阵被变换为2D矩阵，输入维度为啥是这么大？\n",
    "                          stddev=0.1,\n",
    "                          seed=SEED,\n",
    "                          dtype=data_type()))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n",
    "                                                stddev=0.1,\n",
    "                                                seed=SEED,\n",
    "                                                dtype=data_type()))\n",
    "fc2_biases = tf.Variable(tf.constant(\n",
    "      0.1, shape=[NUM_LABELS], dtype=data_type()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN模型构建："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #复构建模型结构，输入数据和初始值，输出模型的第二个全连接层的输出\n",
    "# 模型结构为 conv -> ReLU -> max pool -> conv -> relu ->max pool -> fc1 -> relu -> dropout(train) ->fc2\n",
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D卷积，带有“SAME”填充（即输出要素图与输入的大小相同）。 \n",
    "    # 请注意，{strides}是一个4D数组，其形状与data layout匹配：[image index，y，x，depth]。\n",
    "    conv = tf.nn.conv2d(data, # 形状为 NHWC\n",
    "                        conv1_weights,# kernel，张量，数据类型与data相同，\n",
    "                        strides=[1, 1, 1, 1], # 每个输入维度的滑动窗口的步长\n",
    "                        padding='SAME') # 填充方式SAME/VALID，SAME指增加缺少的列，并填充0；VALID舍弃多余的列\n",
    "    # 偏置和ReLU非线性激活。\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    # 最大池化。\n",
    "    # 内核大小规范{ksize}也遵循数据布局。 这里有一个大小为2的池化窗口和2的步幅。\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                          ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1],\n",
    "                          padding='SAME')\n",
    "    # 将特征图变换为2D矩阵，以将其提供给全连接层\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(\n",
    "        pool,\n",
    "        [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]]) #\n",
    "    # 全连接层. ‘+’操作自动broadcasts偏置bias\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases) # tf.matmul 两个矩阵的内积，类似于reshape.dot(fc1_weight)\n",
    "    # 只在训练阶段添加50% dropout. Dropout还可以scale激活，因此在评估时不需要重新缩放.\n",
    "    if train:\n",
    "      hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练与评估："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_size = train_labels.shape[0]\n",
    "# 训练计算logits和label之间的交叉熵损失.（softmax损失），\n",
    "# logist中有N个数据对应的M个类别的值，与正确的标签（正确的类别）结合计算交叉熵损失\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "  labels=train_labels_node, logits=logits))\n",
    "\n",
    "# 全连接参数的L2 正则化损失\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "              tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# 总损失=样本损失+L2正则化损失\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# 优化器:设置一个变量，每epoch增加一次，控制学习率的下降。\n",
    "batch = tf.Variable(0, dtype=data_type())\n",
    "# 设置学习率，每个epoch衰减一次，使用从0.01开始的指数衰减。\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "                  0.01,                # 初始值\n",
    "                  batch * BATCH_SIZE,  # 当前的globe step，在每个epoch中递增\n",
    "                  train_size,          # 总的Decay step.用来计算学习率的指数，0.01*0.95**（globe/decay）\n",
    "                  0.95,                # 衰减率.每个epoch变为原来的0.95\n",
    "                  staircase=True) #计算离散值，设置为true\n",
    "# 使用简单的动量更新momentum进行优化.动量设置为0.9\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "                                     0.9).minimize(loss,\n",
    "                                                   global_step=batch)\n",
    "\n",
    "# 预测当前训练的minibatch，得到N个数据的对应M类每类的softmax值（sores）\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# 对测试和验证的预测，我们将很少计算它们。\n",
    "eval_prediction = tf.nn.softmax(model(eval_data))\n",
    "\n",
    "# 一个小的实用函数，通过向{eval_data}提供批量数据并从{eval_forecast}中提取结果来评估数据集。\n",
    "# 节省内存，并使其能够在较小的gpu上运行。\n",
    "def eval_in_batches(data, sess):\n",
    "    \"\"\"通过小批量运行数据集来获得数据集的所有预测.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "      raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = np.ndarray(shape=(size, NUM_LABELS), dtype=np.float32)\n",
    "    for begin in xrange(0, size, EVAL_BATCH_SIZE):\n",
    "      end = begin + EVAL_BATCH_SIZE\n",
    "      if end <= size:\n",
    "        predictions[begin:end, :] = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[begin:end, ...]})\n",
    "      else:\n",
    "        batch_predictions = sess.run(\n",
    "            eval_prediction,\n",
    "            feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "        predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建会话，训练和评估模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Step 0 (epoch 0.00), 171.0 ms\n",
      "Minibatch loss: 8.720, learning rate: 0.010000\n",
      "Minibatch error: 87.5%\n",
      "Validation error: 90.5%\n",
      "Step 100 (epoch 0.12), 6.9 ms\n",
      "Minibatch loss: 3.317, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 7.1%\n",
      "Step 200 (epoch 0.23), 5.8 ms\n",
      "Minibatch loss: 3.305, learning rate: 0.010000\n",
      "Minibatch error: 9.4%\n",
      "Validation error: 4.0%\n",
      "Step 300 (epoch 0.35), 5.9 ms\n",
      "Minibatch loss: 3.291, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 3.4%\n",
      "Step 400 (epoch 0.47), 5.9 ms\n",
      "Minibatch loss: 3.292, learning rate: 0.010000\n",
      "Minibatch error: 14.1%\n",
      "Validation error: 2.8%\n",
      "Step 500 (epoch 0.58), 5.8 ms\n",
      "Minibatch loss: 3.256, learning rate: 0.010000\n",
      "Minibatch error: 9.4%\n",
      "Validation error: 2.6%\n",
      "Step 600 (epoch 0.70), 6.0 ms\n",
      "Minibatch loss: 3.091, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 2.5%\n",
      "Step 700 (epoch 0.81), 5.8 ms\n",
      "Minibatch loss: 2.999, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 2.0%\n",
      "Step 800 (epoch 0.93), 5.9 ms\n",
      "Minibatch loss: 3.011, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 2.1%\n",
      "Step 900 (epoch 1.05), 5.8 ms\n",
      "Minibatch loss: 2.922, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.8%\n",
      "Step 1000 (epoch 1.16), 5.9 ms\n",
      "Minibatch loss: 2.856, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 1100 (epoch 1.28), 5.9 ms\n",
      "Minibatch loss: 2.822, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.5%\n",
      "Step 1200 (epoch 1.40), 5.8 ms\n",
      "Minibatch loss: 2.845, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.5%\n",
      "Step 1300 (epoch 1.51), 5.9 ms\n",
      "Minibatch loss: 2.842, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.6%\n",
      "Step 1400 (epoch 1.63), 5.8 ms\n",
      "Minibatch loss: 2.820, learning rate: 0.009500\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.4%\n",
      "Step 1500 (epoch 1.75), 5.9 ms\n",
      "Minibatch loss: 2.784, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 1600 (epoch 1.86), 5.8 ms\n",
      "Minibatch loss: 2.691, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.4%\n",
      "Step 1700 (epoch 1.98), 5.9 ms\n",
      "Minibatch loss: 2.652, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.3%\n",
      "Step 1800 (epoch 2.09), 5.8 ms\n",
      "Minibatch loss: 2.653, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 1900 (epoch 2.21), 5.9 ms\n",
      "Minibatch loss: 2.617, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.3%\n",
      "Step 2000 (epoch 2.33), 5.9 ms\n",
      "Minibatch loss: 2.592, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 2100 (epoch 2.44), 5.8 ms\n",
      "Minibatch loss: 2.617, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 2200 (epoch 2.56), 5.9 ms\n",
      "Minibatch loss: 2.595, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 2300 (epoch 2.68), 5.8 ms\n",
      "Minibatch loss: 2.535, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 2400 (epoch 2.79), 5.9 ms\n",
      "Minibatch loss: 2.497, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 2500 (epoch 2.91), 5.8 ms\n",
      "Minibatch loss: 2.474, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 2600 (epoch 3.03), 5.9 ms\n",
      "Minibatch loss: 2.467, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2700 (epoch 3.14), 5.8 ms\n",
      "Minibatch loss: 2.462, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 2800 (epoch 3.26), 5.9 ms\n",
      "Minibatch loss: 2.463, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2900 (epoch 3.37), 5.8 ms\n",
      "Minibatch loss: 2.490, learning rate: 0.008574\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 3000 (epoch 3.49), 5.9 ms\n",
      "Minibatch loss: 2.414, learning rate: 0.008574\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.1%\n",
      "Step 3100 (epoch 3.61), 5.8 ms\n",
      "Minibatch loss: 2.355, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3200 (epoch 3.72), 5.8 ms\n",
      "Minibatch loss: 2.328, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3300 (epoch 3.84), 5.9 ms\n",
      "Minibatch loss: 2.311, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 3400 (epoch 3.96), 5.8 ms\n",
      "Minibatch loss: 2.285, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3500 (epoch 4.07), 5.8 ms\n",
      "Minibatch loss: 2.339, learning rate: 0.008145\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 3600 (epoch 4.19), 5.9 ms\n",
      "Minibatch loss: 2.253, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3700 (epoch 4.31), 5.8 ms\n",
      "Minibatch loss: 2.246, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 3800 (epoch 4.42), 5.8 ms\n",
      "Minibatch loss: 2.239, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 3900 (epoch 4.54), 5.9 ms\n",
      "Minibatch loss: 2.220, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 4000 (epoch 4.65), 5.8 ms\n",
      "Minibatch loss: 2.228, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 4100 (epoch 4.77), 5.9 ms\n",
      "Minibatch loss: 2.161, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 4200 (epoch 4.89), 5.8 ms\n",
      "Minibatch loss: 2.243, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 4300 (epoch 5.00), 5.8 ms\n",
      "Minibatch loss: 2.128, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4400 (epoch 5.12), 5.9 ms\n",
      "Minibatch loss: 2.130, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4500 (epoch 5.24), 5.8 ms\n",
      "Minibatch loss: 2.183, learning rate: 0.007738\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 4600 (epoch 5.35), 5.9 ms\n",
      "Minibatch loss: 2.125, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 4700 (epoch 5.47), 5.8 ms\n",
      "Minibatch loss: 2.074, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 4800 (epoch 5.59), 5.8 ms\n",
      "Minibatch loss: 2.086, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 4900 (epoch 5.70), 5.9 ms\n",
      "Minibatch loss: 2.035, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5000 (epoch 5.82), 5.8 ms\n",
      "Minibatch loss: 2.085, learning rate: 0.007738\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.8%\n",
      "Step 5100 (epoch 5.93), 5.8 ms\n",
      "Minibatch loss: 2.064, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 5200 (epoch 6.05), 5.9 ms\n",
      "Minibatch loss: 2.040, learning rate: 0.007351\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.9%\n",
      "Step 5300 (epoch 6.17), 5.8 ms\n",
      "Minibatch loss: 1.972, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 5400 (epoch 6.28), 5.8 ms\n",
      "Minibatch loss: 1.959, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5500 (epoch 6.40), 5.9 ms\n",
      "Minibatch loss: 1.983, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 5600 (epoch 6.52), 5.8 ms\n",
      "Minibatch loss: 1.937, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5700 (epoch 6.63), 5.8 ms\n",
      "Minibatch loss: 1.929, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 5800 (epoch 6.75), 5.9 ms\n",
      "Minibatch loss: 1.896, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5900 (epoch 6.87), 5.8 ms\n",
      "Minibatch loss: 1.893, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6000 (epoch 6.98), 5.9 ms\n",
      "Minibatch loss: 1.894, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6100 (epoch 7.10), 5.8 ms\n",
      "Minibatch loss: 1.858, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 6200 (epoch 7.21), 5.8 ms\n",
      "Minibatch loss: 1.843, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6300 (epoch 7.33), 5.9 ms\n",
      "Minibatch loss: 1.865, learning rate: 0.006983\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n",
      "Step 6400 (epoch 7.45), 5.8 ms\n",
      "Minibatch loss: 1.860, learning rate: 0.006983\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.7%\n",
      "Step 6500 (epoch 7.56), 5.9 ms\n",
      "Minibatch loss: 1.812, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6600 (epoch 7.68), 5.8 ms\n",
      "Minibatch loss: 1.849, learning rate: 0.006983\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6700 (epoch 7.80), 5.8 ms\n",
      "Minibatch loss: 1.783, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6800 (epoch 7.91), 5.9 ms\n",
      "Minibatch loss: 1.776, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6900 (epoch 8.03), 5.8 ms\n",
      "Minibatch loss: 1.757, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7000 (epoch 8.15), 5.8 ms\n",
      "Minibatch loss: 1.755, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7100 (epoch 8.26), 5.9 ms\n",
      "Minibatch loss: 1.737, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7200 (epoch 8.38), 5.8 ms\n",
      "Minibatch loss: 1.727, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7300 (epoch 8.49), 5.8 ms\n",
      "Minibatch loss: 1.786, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.6%\n",
      "Step 7400 (epoch 8.61), 5.9 ms\n",
      "Minibatch loss: 1.699, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.7%\n",
      "Step 7500 (epoch 8.73), 5.8 ms\n",
      "Minibatch loss: 1.696, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.7%\n",
      "Step 7600 (epoch 8.84), 5.8 ms\n",
      "Minibatch loss: 1.751, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n",
      "Step 7700 (epoch 8.96), 5.9 ms\n",
      "Minibatch loss: 1.676, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7800 (epoch 9.08), 5.9 ms\n",
      "Minibatch loss: 1.665, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7900 (epoch 9.19), 5.8 ms\n",
      "Minibatch loss: 1.659, learning rate: 0.006302\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n",
      "Step 8000 (epoch 9.31), 5.8 ms\n",
      "Minibatch loss: 1.646, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8100 (epoch 9.43), 5.9 ms\n",
      "Minibatch loss: 1.632, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.7%\n",
      "Step 8200 (epoch 9.54), 5.8 ms\n",
      "Minibatch loss: 1.616, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.7%\n",
      "Step 8300 (epoch 9.66), 5.8 ms\n",
      "Minibatch loss: 1.608, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8400 (epoch 9.77), 6.1 ms\n",
      "Minibatch loss: 1.595, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8500 (epoch 9.89), 5.8 ms\n",
      "Minibatch loss: 1.615, learning rate: 0.006302\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n",
      "Test error: 0.8%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from six.moves import xrange\n",
    "import sys\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "num_epochs = NUM_EPOCHS\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # evaluations之间的步骤数.\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "  \"\"\"返回基于密集预测和稀疏标签的错误率.\"\"\"\n",
    "  return 100.0 - (\n",
    "      100.0 *\n",
    "      np.sum(np.argmax(predictions, 1) == labels) /\n",
    "      predictions.shape[0]) # argmax返回行方向上最大值的索引，即返回每个数据对应的多个类别中得分最高的类别\n",
    "\n",
    "# 创建一个本地session来进行训练\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    # 运行所有初始化来初始化变量\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized!')\n",
    "    # 循环训练\n",
    "    for step in xrange(int(num_epochs * train_size) // BATCH_SIZE): # num_epochs表示整个训练集训练的次数，step表示训练的批次batch\n",
    "      # 计算数据中当前小批量数据的起始位置，即偏移量\n",
    "      offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "      batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "      batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "      # 这个字典将批处理数据(作为一个numpy数组)映射到对应的图中的节点。\n",
    "      feed_dict = {train_data_node: batch_data,\n",
    "                   train_labels_node: batch_labels}\n",
    "      # 运行优化器来更新权重\n",
    "      sess.run(optimizer, feed_dict=feed_dict)\n",
    "      # 一旦达到评估频率（每多少step打印一次数据），打印一些额外的信息\n",
    "      if step % EVAL_FREQUENCY == 0:\n",
    "        # 获取一些额外节点的数据\n",
    "        l, lr, predictions = sess.run([loss, learning_rate, train_prediction],\n",
    "                                      feed_dict=feed_dict)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        start_time = time.time()\n",
    "        print('Step %d (epoch %.2f), %.1f ms' %\n",
    "              (step, float(step) * BATCH_SIZE / train_size,\n",
    "               1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "        print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "        print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
    "        print('Validation error: %.1f%%' % error_rate(\n",
    "            eval_in_batches(validation_data, sess), validation_labels))\n",
    "        sys.stdout.flush()\n",
    "    # 打印结果\n",
    "    test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n",
    "    print('Test error: %.1f%%' % test_error)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
